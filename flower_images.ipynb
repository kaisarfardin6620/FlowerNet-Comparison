{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaisarfardin6620/FlowerNet-Comparison/blob/main/flower_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wZZXYIw-Sfy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import shutil\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, LeakyReLU\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import logging\n",
        "from PIL import Image\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from itertools import cycle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6mFT6tXnL9-"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RauAJ4da-ce7"
      },
      "outputs": [],
      "source": [
        "print(\"--- Mounting Google Drive ---\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. Please ensure you are in a Colab environment and have granted permissions.\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v3q-bV--nc2"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/MyDrive/Dataset/flower_images_split'\n",
        "output_dir = '/content/drive/MyDrive/Dataset/flower_images_split'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Base dataset path: {base_path}\")\n",
        "print(f\"Output directory: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_6QelHl4C2D"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "CHANNEL_NUM = 3\n",
        "BATCH_SIZE = 32\n",
        "RANDOM_SEED = 42\n",
        "EPOCHS = 100\n",
        "input_shape = (IMG_WIDTH, IMG_HEIGHT, CHANNEL_NUM)\n",
        "\n",
        "print(f\"Image dimensions: {IMG_WIDTH}x{IMG_HEIGHT}x{CHANNEL_NUM}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Training epochs: {EPOCHS}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxz09KCAnVy3"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqt_w7Cr1o6C"
      },
      "outputs": [],
      "source": [
        "def prepare_and_analyze_dataset(base_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "\n",
        "    print(\"\\n--- Dataset Preparation and Overview ---\")\n",
        "\n",
        "    if not os.path.isdir(base_path):\n",
        "        print(f\"Error: Base path '{base_path}' does not exist. Please check your Drive path.\")\n",
        "        return\n",
        "\n",
        "    train_path = os.path.join(base_path, 'train')\n",
        "    val_path = os.path.join(base_path, 'val')\n",
        "    test_path = os.path.join(base_path, 'test')\n",
        "\n",
        "    has_train = os.path.isdir(train_path)\n",
        "    has_val = os.path.isdir(val_path)\n",
        "    has_test = os.path.isdir(test_path)\n",
        "\n",
        "    if not has_train and not has_val and not has_test:\n",
        "        print(\"No 'train', 'val', 'test' folders found. Splitting raw class folders into splits...\")\n",
        "        class_folders = [name for name in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, name))]\n",
        "\n",
        "        if not class_folders:\n",
        "            print(f\"Error: No class folders found in '{base_path}'. Cannot split dataset.\")\n",
        "            return\n",
        "\n",
        "        os.makedirs(train_path, exist_ok=True)\n",
        "        os.makedirs(val_path, exist_ok=True)\n",
        "        os.makedirs(test_path, exist_ok=True)\n",
        "\n",
        "        for class_name in class_folders:\n",
        "            class_source_path = os.path.join(base_path, class_name)\n",
        "            images = [f for f in os.listdir(class_source_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "            random.shuffle(images)\n",
        "\n",
        "            num_images = len(images)\n",
        "            num_train = int(train_ratio * num_images)\n",
        "            num_val = int(val_ratio * num_images)\n",
        "            num_test = num_images - num_train - num_val\n",
        "\n",
        "            num_test = int(test_ratio * num_images)\n",
        "            if (num_train + num_val + num_test) > num_images:\n",
        "                num_test = num_images - num_train - num_val\n",
        "                if num_test < 0:\n",
        "                    num_val = int(val_ratio * num_images)\n",
        "                    num_train = num_images - num_val - num_test\n",
        "                    if num_train < 0:\n",
        "                        num_train = 0\n",
        "\n",
        "\n",
        "            train_images = images[:num_train]\n",
        "            val_images = images[num_train : num_train + num_val]\n",
        "            test_images = images[num_train + num_val : num_train + num_val + num_test]\n",
        "\n",
        "            os.makedirs(os.path.join(train_path, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(val_path, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(test_path, class_name), exist_ok=True)\n",
        "\n",
        "            for img in train_images:\n",
        "                shutil.move(os.path.join(class_source_path, img), os.path.join(train_path, class_name, img))\n",
        "            for img in val_images:\n",
        "                shutil.move(os.path.join(class_source_path, img), os.path.join(val_path, class_name, img))\n",
        "            for img in test_images:\n",
        "                shutil.move(os.path.join(class_source_path, img), os.path.join(test_path, class_name, img))\n",
        "\n",
        "            if not os.listdir(class_source_path):\n",
        "                os.rmdir(class_source_path)\n",
        "\n",
        "        print(\"Dataset split into 'train', 'val', 'test' successfully.\")\n",
        "\n",
        "    elif has_train and has_test and not has_val:\n",
        "        print(\"'train' and 'test' folders found, but 'val' is missing. Creating 'val' folder...\")\n",
        "        os.makedirs(val_path, exist_ok=True)\n",
        "\n",
        "        print(\"Moving images from 'train' to create 'val' split...\")\n",
        "        train_classes = [name for name in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, name))]\n",
        "\n",
        "        for class_name in train_classes:\n",
        "            train_class_path = os.path.join(train_path, class_name)\n",
        "            val_class_path = os.path.join(val_path, class_name)\n",
        "            os.makedirs(val_class_path, exist_ok=True)\n",
        "\n",
        "            images = [f for f in os.listdir(train_class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "            random.shuffle(images)\n",
        "\n",
        "            num_val_to_move = int(val_ratio / (train_ratio + val_ratio) * len(images))\n",
        "            if num_val_to_move == 0 and len(images) > 0:\n",
        "                num_val_to_move = 1\n",
        "\n",
        "            val_images_to_move = images[:num_val_to_move]\n",
        "\n",
        "            for img in val_images_to_move:\n",
        "                shutil.move(os.path.join(train_class_path, img), os.path.join(val_class_path, img))\n",
        "        print(\"'val' split created successfully.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Existing 'train', 'val', 'test' folders found. Proceeding with analysis.\")\n",
        "\n",
        "    split_totals = {}\n",
        "    for split_name, current_split_path in [('train', train_path), ('val', val_path), ('test', test_path)]:\n",
        "        if os.path.isdir(current_split_path):\n",
        "            classes = [name for name in os.listdir(current_split_path) if os.path.isdir(os.path.join(current_split_path, name))]\n",
        "            total_images = 0\n",
        "            for class_name in classes:\n",
        "                class_dir = os.path.join(current_split_path, class_name)\n",
        "                if os.path.isdir(class_dir):\n",
        "                    total_images += len([f for f in os.listdir(class_dir)\n",
        "                                         if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "\n",
        "            split_totals[split_name] = {'classes': len(classes), 'images': total_images}\n",
        "        else:\n",
        "            print(f\"Warning: Split folder '{split_name.upper()}' not found after preparation. Skipping this split in summary.\")\n",
        "\n",
        "    print(\"\\n--- Final Dataset Overview ---\")\n",
        "    for split, data in split_totals.items():\n",
        "        print(f\"{split.upper()} Split:\")\n",
        "        print(f\"  Number of classes: {data['classes']}\")\n",
        "        print(f\"  Total images: {data['images']}\")\n",
        "        print()\n",
        "\n",
        "prepare_and_analyze_dataset(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXtKicno1O8l"
      },
      "outputs": [],
      "source": [
        "class_counts_overall = {}\n",
        "if os.path.isdir(base_path):\n",
        "    for split_name in ['train', 'val', 'test']:\n",
        "        split_path = os.path.join(base_path, split_name)\n",
        "        if os.path.isdir(split_path):\n",
        "            for class_name in os.listdir(split_path):\n",
        "                class_path = os.path.join(split_path, class_name)\n",
        "                if os.path.isdir(class_path):\n",
        "                    image_count = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "                    class_counts_overall[class_name] = class_counts_overall.get(class_name, 0) + image_count\n",
        "\n",
        "if class_counts_overall:\n",
        "    min_count_overall = min(class_counts_overall.values()) if class_counts_overall else 0\n",
        "    max_count_overall = max(class_counts_overall.values()) if class_counts_overall else 0\n",
        "    class_counts_overall = dict(sorted(class_counts_overall.items(), key=lambda item: item[1], reverse=True))\n",
        "    print(\"Total number of classes (overall):\", len(class_counts_overall))\n",
        "    print(\"Total number of images (overall):\", sum(class_counts_overall.values()))\n",
        "    print(\"Class-wise image counts (overall):\")\n",
        "    for class_name, count in class_counts_overall.items():\n",
        "        print(f\"  {class_name}: {count} images\")\n",
        "    print(f\"\\nMinimum number of images in a class (overall): {min_count_overall}\")\n",
        "    print(f\"Maximum number of images in a class (overall): {max_count_overall}\")\n",
        "else:\n",
        "    print(f\"No class folders found within the split directories under '{base_path}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8euf-E33JUL"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Defining Callbacks ---\")\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=os.path.join(output_dir, 'flower_DenseNet121.keras'),\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < 5:\n",
        "        return lr\n",
        "    elif epoch < 10:\n",
        "        return lr * 0.5\n",
        "    elif epoch < 15:\n",
        "        return lr * 0.2\n",
        "    else:\n",
        "        return lr * 0.1\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n",
        "callbacks = [early_stopping, model_checkpoint, lr_scheduler]\n",
        "print(\"Callbacks defined: EarlyStopping, ModelCheckpoint, LearningRateScheduler\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6WQ68Px567u"
      },
      "outputs": [],
      "source": [
        "def create_dataframe_from_folder(base_dir):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "    data_sets = []\n",
        "\n",
        "    for split_name in ['train', 'test', 'val']:\n",
        "        split_path = os.path.join(base_dir, split_name)\n",
        "        if not os.path.isdir(split_path):\n",
        "            print(f\"Warning: Split folder '{split_path}' not found. Skipping this split.\")\n",
        "            continue\n",
        "\n",
        "        for class_name in os.listdir(split_path):\n",
        "            class_path = os.path.join(split_path, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                for file in os.listdir(class_path):\n",
        "                    if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "                        full_path = os.path.join(class_path, file)\n",
        "                        relative_path = os.path.relpath(full_path, base_dir)\n",
        "\n",
        "                        filepaths.append(relative_path)\n",
        "                        labels.append(class_name)\n",
        "                        data_sets.append(split_name)\n",
        "\n",
        "    new_df = pd.DataFrame({\n",
        "        'filepaths': filepaths,\n",
        "        'labels': labels,\n",
        "        'image_path': [os.path.join(base_dir, fp) for fp in filepaths],\n",
        "        'data set': data_sets\n",
        "    })\n",
        "\n",
        "    return new_df\n",
        "\n",
        "print(f\"\\n--- Scanning '{base_path}' to create initial DataFrame from folder structure ---\")\n",
        "df = create_dataframe_from_folder(base_path)\n",
        "\n",
        "if df.empty:\n",
        "    raise ValueError(f\"No image files found in '{base_path}'. Please check your base_path and folder structure.\")\n",
        "print(f\"DataFrame created with {len(df)} entries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15CQ-q9X7vi3"
      },
      "outputs": [],
      "source": [
        "train_df_original = df[df['data set'] == 'train'].copy()\n",
        "test_df_original = df[df['data set'] == 'test'].copy()\n",
        "validation_df_original = df[df['data set'] == 'val'].copy()\n",
        "print(\"DataFrames for train, validation, and test splits created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BabFGtcT7w2d"
      },
      "outputs": [],
      "source": [
        "train_df_original = train_df_original.rename(columns={'labels': 'label'})\n",
        "test_df_original = test_df_original.rename(columns={'labels': 'label'})\n",
        "validation_df_original = validation_df_original.rename(columns={'labels': 'label'})\n",
        "print(\"DataFrames for train, validation, and test splits created and columns renamed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHfOWTOF70eg"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Detailed Data Distribution (Text Summary) ---\")\n",
        "\n",
        "print(\"\\nTRAIN Split:\")\n",
        "train_class_distribution = train_df_original['label'].value_counts().sort_index()\n",
        "print(f\"Number of classes: {len(train_class_distribution)}\")\n",
        "print(f\"Total images: {len(train_df_original)}\")\n",
        "print(\"Class-wise image counts:\")\n",
        "for class_name, count in train_class_distribution.items():\n",
        "    print(f\"  {class_name}: {count} images\")\n",
        "print(f\"Minimum images per class: {train_class_distribution.min()} images\")\n",
        "print(f\"Maximum images per class: {train_class_distribution.max()} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJiQ1zhe72Om"
      },
      "outputs": [],
      "source": [
        "print(\"\\nVAL Split:\")\n",
        "val_class_distribution = validation_df_original['label'].value_counts().sort_index()\n",
        "print(f\"Number of classes: {len(val_class_distribution)}\")\n",
        "print(f\"Total images: {len(validation_df_original)}\")\n",
        "print(\"Class-wise image counts:\")\n",
        "for class_name, count in val_class_distribution.items():\n",
        "    print(f\"  {class_name}: {count} images\")\n",
        "print(f\"Minimum images per class: {val_class_distribution.min()} images\")\n",
        "print(f\"Maximum images per class: {val_class_distribution.max()} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIemmrH874TU"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTEST Split:\")\n",
        "test_class_distribution = test_df_original['label'].value_counts().sort_index()\n",
        "print(f\"Number of classes: {len(test_class_distribution)}\")\n",
        "print(f\"Total images: {len(test_df_original)}\")\n",
        "print(\"Class-wise image counts:\")\n",
        "for class_name, count in test_class_distribution.items():\n",
        "    print(f\"  {class_name}: {count} images\")\n",
        "print(f\"Minimum images per class: {test_class_distribution.min()} images\")\n",
        "print(f\"Maximum images per class: {test_class_distribution.max()} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfkVO-EG760L"
      },
      "outputs": [],
      "source": [
        "temp_datagen = ImageDataGenerator()\n",
        "temp_generator = temp_datagen.flow_from_dataframe(\n",
        "    dataframe=df.rename(columns={'labels': 'label'}),\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=1,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJNPOoDV4vWc"
      },
      "outputs": [],
      "source": [
        "overall_class_distribution = df['labels'].value_counts()\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x=overall_class_distribution.index, y=overall_class_distribution.values, palette='cubehelix', hue=overall_class_distribution.index, legend=False)\n",
        "plt.title('Overall Distribution of Classes Across All Data (Train, Test, Val Combined)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu1doYsx8Qpl"
      },
      "outputs": [],
      "source": [
        "train_class_distribution = train_df_original['label'].value_counts()\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x=train_class_distribution.index, y=train_class_distribution.values, palette='viridis', hue=train_class_distribution.index, legend=False)\n",
        "plt.title('Distribution of Classes in Training Set')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"\\nTraining Set Class Distribution (from plot data):\")\n",
        "print(train_class_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIIBtM1d8UqG"
      },
      "outputs": [],
      "source": [
        "val_class_distribution = validation_df_original['label'].value_counts()\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x=val_class_distribution.index, y=val_class_distribution.values, palette='magma', hue=val_class_distribution.index, legend=False)\n",
        "plt.title('Distribution of Classes in Validation Set')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"\\nValidation Set Class Distribution (from plot data):\")\n",
        "print(val_class_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zloLekA8XTE"
      },
      "outputs": [],
      "source": [
        "test_class_distribution = test_df_original['label'].value_counts()\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.barplot(x=test_class_distribution.index, y=test_class_distribution.values, palette='cividis', hue=test_class_distribution.index, legend=False)\n",
        "plt.title('Distribution of Classes in Test Set')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=90, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"\\nTest Set Class Distribution (from plot data):\")\n",
        "print(test_class_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhXJRcZE8ZSu"
      },
      "outputs": [],
      "source": [
        "data_volume = pd.DataFrame({\n",
        "    'Dataset': ['Train', 'Test', 'Validation'],\n",
        "    'Count': [len(train_df_original), len(test_df_original), len(validation_df_original)]\n",
        "})\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Dataset', y='Count', data=data_volume, palette='coolwarm', hue='Dataset', legend=False)\n",
        "plt.title('Volume of Train, Test, and Validation Datasets')\n",
        "plt.xlabel('Dataset Type')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_L3MRXJ98s3"
      },
      "outputs": [],
      "source": [
        "unique_classes_count = pd.DataFrame({\n",
        "    'Dataset': ['Train', 'Test', 'Validation'],\n",
        "    'Unique Classes': [train_df_original['label'].nunique(), test_df_original['label'].nunique(), validation_df_original['label'].nunique()]\n",
        "})\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Dataset', y='Unique Classes', data=unique_classes_count, palette='rocket', hue='Dataset', legend=False)\n",
        "plt.title('Number of Unique Classes per Dataset Split')\n",
        "plt.xlabel('Dataset Type')\n",
        "plt.ylabel('Count of Unique Classes')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY_Gb2um9_3b"
      },
      "outputs": [],
      "source": [
        "dataset_type_distribution = df['data set'].value_counts()\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.pie(dataset_type_distribution, labels=dataset_type_distribution.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))\n",
        "plt.title('Distribution of Samples by Dataset Type')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_E3AX2eHHEX-"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Class Distribution in Training Set (with counts) ---\")\n",
        "\n",
        "train_class_distribution = train_df_original['label'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "ax = sns.barplot(x=train_class_distribution.index, y=train_class_distribution.values, palette='viridis', hue=train_class_distribution.index, legend=False)\n",
        "plt.title('Distribution of Classes in Training Set')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.tight_layout()\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYq-QNO2HI8S"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Visualizing Image File Size Distribution ---\")\n",
        "\n",
        "def get_file_size(image_path):\n",
        "    try:\n",
        "        return os.path.getsize(image_path)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not get file size for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "df['file_size'] = df['image_path'].apply(get_file_size)\n",
        "\n",
        "df_with_sizes = df.dropna(subset=['file_size']).copy()\n",
        "\n",
        "if not df_with_sizes.empty:\n",
        "    df_with_sizes['file_size_kb'] = df_with_sizes['file_size'] / 1024\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df_with_sizes['file_size_kb'], bins=50, kde=True)\n",
        "    plt.title('Distribution of Image File Sizes (KB)')\n",
        "    plt.xlabel('File Size (KB)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nSummary statistics for image file sizes (bytes):\")\n",
        "    print(df_with_sizes['file_size'].describe())\n",
        "else:\n",
        "     print(\"No image file size data could be retrieved to plot.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Visualizing Class Distribution by Split (Stacked Bar Chart) ---\")\n",
        "\n",
        "df_combined = pd.concat([\n",
        "    train_df_original.assign(split='Train'),\n",
        "    validation_df_original.assign(split='Validation'),\n",
        "    test_df_original.assign(split='Test')\n",
        "])\n",
        "\n",
        "class_split_counts = pd.crosstab(df_combined['label'], df_combined['split'])\n",
        "\n",
        "ax = class_split_counts.plot(kind='bar', stacked=True, figsize=(12, 7), colormap='viridis')\n",
        "\n",
        "plt.title('Class Distribution by Dataset Split (Stacked Bar Chart)')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Dataset Split')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dW-5oY9sRfFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Visualizing Image Dimensions Distribution ---\")\n",
        "\n",
        "def get_image_dimensions(image_path):\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            return img.size\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Could not get dimensions for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "df['dimensions'] = df['image_path'].apply(get_image_dimensions)\n",
        "\n",
        "df_with_dimensions = df.dropna(subset=['dimensions']).copy()\n",
        "\n",
        "if not df_with_dimensions.empty:\n",
        "    df_with_dimensions['width'] = df_with_dimensions['dimensions'].apply(lambda x: x[0])\n",
        "    df_with_dimensions['height'] = df_with_dimensions['dimensions'].apply(lambda x: x[1])\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df_with_dimensions['width'], bins=50, kde=True)\n",
        "    plt.title('Distribution of Image Widths')\n",
        "    plt.xlabel('Width (pixels)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.histplot(df_with_dimensions['height'], bins=50, kde=True)\n",
        "    plt.title('Distribution of Image Heights')\n",
        "    plt.xlabel('Height (pixels)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nSummary statistics for image dimensions:\")\n",
        "    print(df_with_dimensions[['width', 'height']].describe())\n",
        "\n",
        "else:\n",
        "    print(\"No image dimension data could be retrieved to plot.\")"
      ],
      "metadata": {
        "id": "8-C18tMcRnO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8-7L61M-CNN"
      },
      "outputs": [],
      "source": [
        "def display_sample_images(dataframe, num_classes=5, images_per_class=3):\n",
        "    unique_classes = dataframe['label'].unique()\n",
        "    if len(unique_classes) < num_classes:\n",
        "        num_classes = len(unique_classes)\n",
        "\n",
        "    selected_classes = np.random.choice(unique_classes, num_classes, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(images_per_class * 3, num_classes * 3))\n",
        "    plt.suptitle('Sample Images from Different Classes', fontsize=18, y=1.02)\n",
        "\n",
        "    for i, class_name in enumerate(selected_classes):\n",
        "        class_images = dataframe[dataframe['label'] == class_name]['image_path'].sample(min(images_per_class, len(dataframe[dataframe['label'] == class_name])), random_state=42)\n",
        "\n",
        "        for j, image_path in enumerate(class_images):\n",
        "            ax = plt.subplot(num_classes, images_per_class, i * images_per_class + j + 1)\n",
        "            try:\n",
        "                img = Image.open(image_path)\n",
        "                ax.imshow(img)\n",
        "                ax.set_title(f\"{class_name}\", fontsize=10)\n",
        "                ax.axis('off')\n",
        "            except Exception as e:\n",
        "                ax.set_title(f\"Error loading {class_name}\", fontsize=10)\n",
        "                ax.axis('off')\n",
        "                logging.error(f\"Could not load image {image_path}: {e}\")\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Displaying Sample Images ---\")\n",
        "display_sample_images(train_df_original, num_classes=5, images_per_class=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uthAxFT-FBf"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Setting up ImageDataGenerators ---\")\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    brightness_range=[0.7, 1.3],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    channel_shift_range=150,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df_original,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "validation_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=validation_df_original,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df_original,\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI2n7f48AeKp"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = len(train_generator.class_indices)\n",
        "print(f\"\\nFinal NUM_CLASSES for model output: {NUM_CLASSES}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9VddSfIoMFj"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Class Serialization (Class Name to Index Mapping) ---\")\n",
        "sorted_class_indices = sorted(train_generator.class_indices.items(), key=lambda item: item[1])\n",
        "class_names = [item[0] for item in sorted_class_indices]\n",
        "for class_name, index in sorted_class_indices:\n",
        "    print(f\"  {class_name}: {index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3pXvapv_B8l"
      },
      "outputs": [],
      "source": [
        "def build_model(num_classes, input_shape=(IMG_WIDTH, IMG_HEIGHT, CHANNEL_NUM)):\n",
        "    print(f\"Building model with input_shape: {input_shape} and {num_classes} classes.\")\n",
        "    base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=input_shape, pooling='avg')\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    print(\"InceptionResNetV2 base model loaded and frozen.\")\n",
        "\n",
        "    x = base_model.output\n",
        "\n",
        "    x = Dense(1024, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "    x = LeakyReLU(negative_slope=0.01)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Dense(512, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "    x = LeakyReLU(negative_slope=0.01)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Dense(256, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "    x = LeakyReLU(negative_slope=0.01)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    x = Dense(128, kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "    x = LeakyReLU(negative_slope=0.01)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    print(\"Custom classification head added.\")\n",
        "    return model, base_model\n",
        "\n",
        "print(\"\\n--- Building the Model ---\")\n",
        "model, base_model = build_model(NUM_CLASSES, input_shape)\n",
        "print(\"Model built successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Compiling the Model ---\")\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "print(\"Model compiled successfully.\")"
      ],
      "metadata": {
        "id": "FX6hErZsLnb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "_GjyvUpmLqCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fda5eb5"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Starting Model Training (Head Only, Base Frozen) ---\")\n",
        "try:\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    print(\"\\n--- Model Training Finished ---\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during model training: {e}\")\n",
        "    history = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5Q91ht6EHZW"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Evaluating Model on Test Set ---\")\n",
        "if history:\n",
        "    test_loss, test_accuracy = model.evaluate(test_generator, verbose=1)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "else:\n",
        "    print(\"Model training failed, skipping evaluation.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Plotting Learning Rate over Epochs ---\")\n",
        "\n",
        "if history and 'lr' in history.history:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['lr'])\n",
        "    plt.title('Learning Rate over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Learning rate history not available. Please ensure the LearningRateScheduler callback was used and training completed successfully.\")"
      ],
      "metadata": {
        "id": "fVki2g-ORW3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Generating Performance Plots ---\")\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T8wAt2cNMUHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqZPfELcorjY"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Generating Confusion Matrix ---\")\n",
        "\n",
        "test_generator.reset()\n",
        "Y_pred = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
        "y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "y_true = test_generator.classes\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Generating Classification Report ---\")\n",
        "report = classification_report(y_true, y_pred_classes, target_names=class_names, zero_division=0)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dbd3b9d"
      },
      "source": [
        "print(\"\\n--- Generating ROC Curve and AUC (Micro-average) ---\")\n",
        "\n",
        "test_generator.reset()\n",
        "Y_pred = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
        "y_true = test_generator.classes\n",
        "y_true_one_hot = to_categorical(y_true, num_classes=NUM_CLASSES)\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true_one_hot.ravel(), Y_pred.ravel())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Micro-average Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Generating ROC Curves for Each Class (One-vs-Rest) ---\")\n",
        "plt.figure(figsize=(12, 10))\n",
        "colors = cycle(['blue', 'red', 'green', 'cyan', 'magenta', 'yellow', 'black', 'purple', 'pink', 'brown'])\n",
        "for i, color in zip(range(NUM_CLASSES), colors):\n",
        "    fpr, tpr, _ = roc_curve(y_true_one_hot[:, i], Y_pred[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=color, lw=2,\n",
        "              label='ROC curve of class {0} (area = {1:0.2f})'.format(class_names[i], roc_auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Each Class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vmFuAMniR4ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "496ee236"
      },
      "source": [
        "print(\"\\n--- Generating Precision-Recall Curve and Average Precision (Micro-average) ---\")\n",
        "\n",
        "test_generator.reset()\n",
        "Y_pred = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
        "y_true = test_generator.classes\n",
        "y_true_one_hot = to_categorical(y_true, num_classes=NUM_CLASSES)\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_true_one_hot.ravel(), Y_pred.ravel())\n",
        "average_precision = average_precision_score(y_true_one_hot, Y_pred, average=\"micro\")\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (area = {average_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Micro-average Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Generating Precision-Recall Curves for Each Class (One-vs-Rest) ---\")\n",
        "plt.figure(figsize=(12, 10))\n",
        "colors = cycle(['blue', 'red', 'green', 'cyan', 'magenta', 'yellow', 'black', 'purple', 'pink', 'brown'])\n",
        "for i, color in zip(range(NUM_CLASSES), colors):\n",
        "    precision, recall, _ = precision_recall_curve(y_true_one_hot[:, i], Y_pred[:, i])\n",
        "    average_precision = average_precision_score(y_true_one_hot[:, i], Y_pred[:, i])\n",
        "    plt.plot(recall, precision, color=color, lw=2,\n",
        "              label='Precision-Recall curve of class {0} (area = {1:0.2f})'.format(class_names[i], average_precision))\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Each Class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-7Xe1iAOSdjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JkQw4FVNSr0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggTXIj_FSrwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wMUuLxG7Srtr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+DiFlJbufvCo+vJ19CUrW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}